{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "97ab1d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9c977ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "status = [\"Open\",\"Closed\",\"Resolved\",\"Waiting for Customer\",\"Waiting for Third Party\",\"Pending\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62cb6883",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = \"example.json\"\n",
    "with open(json_path) as json_file:\n",
    "    json_dict = json.load(json_file)\n",
    "\n",
    "# attributes = json_dict['events'][0]['attributes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21c2ab5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/24 21:34:44 WARN Utils: Your hostname, Phuongs-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.102 instead (on interface en0)\n",
      "22/11/24 21:34:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/24 21:34:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('ticket_processing').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ff07f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- activities_data: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- activity: struct (nullable = true)\n",
      " |    |    |    |-- agent_id: long (nullable = true)\n",
      " |    |    |    |-- category: string (nullable = true)\n",
      " |    |    |    |-- contacted_customer: boolean (nullable = true)\n",
      " |    |    |    |-- group: string (nullable = true)\n",
      " |    |    |    |-- issue_type: string (nullable = true)\n",
      " |    |    |    |-- note: struct (nullable = true)\n",
      " |    |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |    |-- type: long (nullable = true)\n",
      " |    |    |    |-- priority: long (nullable = true)\n",
      " |    |    |    |-- product: string (nullable = true)\n",
      " |    |    |    |-- requester: long (nullable = true)\n",
      " |    |    |    |-- shipment_date: string (nullable = true)\n",
      " |    |    |    |-- shipping_address: string (nullable = true)\n",
      " |    |    |    |-- source: long (nullable = true)\n",
      " |    |    |    |-- status: string (nullable = true)\n",
      " |    |    |-- performed_at: string (nullable = true)\n",
      " |    |    |-- performer_id: long (nullable = true)\n",
      " |    |    |-- performer_type: string (nullable = true)\n",
      " |    |    |-- ticket_id: long (nullable = true)\n",
      " |-- metadata: struct (nullable = true)\n",
      " |    |-- activities_count: long (nullable = true)\n",
      " |    |-- end_at: string (nullable = true)\n",
      " |    |-- start_at: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_json = spark.read.json(json_path, multiLine= True)\n",
    "df_json.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "09349867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nested_json(df):\n",
    "    col_list = []\n",
    "    \n",
    "    for name in df.schema.names:\n",
    "        # if item is structtype, then get field names inside item\n",
    "        if isinstance(df.schema[name].dataType, StructType):\n",
    "            for field in df.schema[name].dataType.fields:\n",
    "                col_list.append(col(\".\".join([name,field.name])).alias(\"_\".join([name,field.name])))\n",
    "                \n",
    "                \n",
    "        # if item is Array type, then make new df and explode/separate array\n",
    "        elif isinstance(df.schema[name].dataType, ArrayType):\n",
    "            df = df.withColumn(name,explode(name).alias(name))\n",
    "            col_list.append(name)\n",
    "        # if other types, then just add\n",
    "        else:\n",
    "            col_list.append(name)\n",
    "    df = df.select(col_list)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "a2ab348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contain_struct(element):\n",
    "    '''\n",
    "    Return True if one or more element in input contains StructType\n",
    "    False if no StructType element is found\n",
    "    '''\n",
    "    # list all fields that are of StructType (contain sub information)\n",
    "    struct_list = [field.name for field in element.dataType.fields \\\n",
    "                        if isinstance(field.dataType, StructType)]\n",
    "    \n",
    "    return True if len(struct_list) > 0 else False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "2736c937",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_df = df_json.select('activities_data')\n",
    "activity_df = activity_df.withColumn('activities_data',explode('activities_data'))\n",
    "col_list = [\".\".join(['activities_data', field.name]) \\\n",
    "            for field in activity_df.schema['activities_data'].dataType.fields]\n",
    "activity_df = activity_df.select(col_list)\n",
    "\n",
    "col_list = [field.name for field in activity_df.schema.fields if isinstance(field.dataType,StructType) == False]\n",
    "struct_list = [field for field in activity_df.schema.fields if isinstance(field.dataType,StructType)]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "4a3801fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['activity']"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[field.name for field in activity_df.schema.fields if isinstance(field.dataType,StructType)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "0143fb44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['activities_data.performed_at',\n",
       " 'activities_data.performer_id',\n",
       " 'activities_data.performer_type',\n",
       " 'activities_data.ticket_id']"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look for elements that aren't structType and combine in a list\n",
    "col_list = [\".\".join(['activities_data', field.name]) for field in test_df.schema['activities_data'].dataType.fields if \\\n",
    "           isinstance(field.dataType, StructType) == False]\n",
    "\n",
    "test_df.withColumn('activity_explode',explode(activity))\n",
    "            \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "f6cd9fb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Column 'activity' does not exist. Did you mean one of the following? [activities_data];\n'Project [activities_data#931, explode('activity) AS activity_explode#933]\n+- Project [activities_data#931]\n   +- Generate explode(activities_data#73), false, [activities_data#931]\n      +- Project [activities_data#73]\n         +- Relation [activities_data#73,metadata#74] json\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/nd/pgsq9c7134jfmgn899bxhdjc0000gn/T/ipykernel_21119/2208405930.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'activity_explode'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'activity'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   3034\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3035\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3036\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3038\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Column 'activity' does not exist. Did you mean one of the following? [activities_data];\n'Project [activities_data#931, explode('activity) AS activity_explode#933]\n+- Project [activities_data#931]\n   +- Generate explode(activities_data#73), false, [activities_data#931]\n      +- Project [activities_data#73]\n         +- Relation [activities_data#73,metadata#74] json\n"
     ]
    }
   ],
   "source": [
    "test_df.withColumn('activity_explode',explode('activity'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5bc4a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
